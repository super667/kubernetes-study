
# etcd问题记录

# raft协议中

1. lead选举
2. 日志复制

## etcd选举流程

## etcd日志复制流程

https://andblog.cn/3160

## 1.如果一个日志完整度较高的节点因为自己启动时间比其他节点的长，没能最先发起精选，其他节点当上leader后同步自己的日志岂不是冲突了？

这个日志完整度相对较高的节点，投票时有竞选规则安全限制，如果它的节点比较新会拒绝投票，至于最终先发起选举的节点能否赢得选举，要看其他节点数据情况，如果多数节点的数据比它新，那么先发起选举的节点就无法获得多数选票，如果5个节点中，只有一个节点数据比较长，那的确会被覆盖，但是这是安全的，说明这个数据并未被集群节点多数确认

## 如果leader将消息同步到大多数节点后，返回给客户端途中，leader宕机，客户端没有收到消息怎么办？

需要靠客户端实现幂等，通过业务唯一id等

## 哪些场景会出现Follower日志和Leader冲突

etcd特别依赖磁盘I/O性能，日志条目需要同步持久化到磁盘，当心跳间隔是100ms,选举超时是1s, 如果fsync 日志条目WAL耗时不稳定、波动很大、超过1秒，那集群就会频繁选举。

## follower节点如何删除无效的日志

leader 处理不一致是通过强制 follower 直接复制自己的日志来解决。因此在 follower 中的冲突的日志条目会被 leader 的日志覆盖。leader 会记录 follower 的日志复制进度 nextIndex，如果 follower 在追加日志时一致性检查失败，就会拒绝请求，此时 leader 就会减小 nextIndex 值并进行重试，最终在某个位置让 follower 跟 leader 一致。

这里我补充下为什么 WAL 日志模块只通过追加，也能删除已持久化冲突的日志条目呢？ 其实这里 etcd 在实现上采用了一些比较有技巧的方法，在 WAL 日志中的确没删除废弃的日志条目，你可以在其中搜索到冲突的日志条目。只是 etcd 加载 WAL 日志时，发现一个 raft log index 位置上有多个日志条目的时候，会通过覆盖的方式，将最后写入的日志条目追加到 raft log 中，实现了删除冲突日志条目效果，

## 在数据敏感度要求比较高的场景，如果主节点写入一个key，并且同步到大多数节点。节点c因为磁盘io压力大，没有及时把wal日志里的数据及时同步到sanpshot中，会不会出现读不一致？

串行读：直接读状态机数据返回、无需通过 Raft 协议与集群进行交互的模式，在 etcd 里叫做串行 (Serializable) 读，它具有低延时、高吞吐量的特点，适合对数据一致性要求不高的场景。

线性读之readindex：一旦一个值更新成功，随后任何通过线性读的 client 都能及时访问到。虽然集群中有多个节点，但 client 通过线性读就如访问一个节点一样。etcd 默认读模式是线性读，因为它需要经过 Raft 协议模块，反应的是集群共识，因此在延时和吞吐量上相比串行读略差一点，适用于对数据一致性要求高的场景